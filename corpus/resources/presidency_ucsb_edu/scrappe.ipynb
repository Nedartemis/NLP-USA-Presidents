{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Thread, local\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(category: str) -> None:\n",
    "    output_file = category + \".csv\"\n",
    "    if os.path.exists(output_file):\n",
    "        return\n",
    "\n",
    "    output: List[dict] = []\n",
    "    for file in tqdm(os.listdir(category)):\n",
    "        with open(os.path.join(category, file), \"r\") as reader:\n",
    "            content = json.load(reader)\n",
    "        for sample in content[\"content\"]:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"category\": category,\n",
    "                    \"name\": sample[\"name\"],\n",
    "                    \"date\": sample[\"date\"],\n",
    "                    \"text\": sample[\"text\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    header = [\"category\", \"name\", \"date\", \"text\"]\n",
    "    with open(output_file, \"w\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for row in output:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '{'date', 'datetime64[h]'}' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvetoes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatetime64[h]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:6640\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6634\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6635\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6636\u001b[0m     ]\n\u001b[1;32m   6638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6639\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6640\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6641\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:231\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    225\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an instance of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got the class instead. Try instantiating \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 231\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, NumpyEADtype):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# Ensure we don't end up with a NumpyExtensionArray\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1645\u001b[0m, in \u001b[0;36mpandas_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;66;03m# GH#51523 - Series.astype(np.integer) doesn't show\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;66;03m# numpy deprecation warning of np.integer\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m         \u001b[38;5;66;03m# Hence enabling DeprecationWarning\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m-> 1645\u001b[0m         npdtype \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;66;03m# np.dtype uses `eval` which can raise SyntaxError\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '{'date', 'datetime64[h]'}' as a data type"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"vetoes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18334/18334 [04:11<00:00, 72.86it/s]\n",
      "100%|██████████| 6444/6444 [00:25<00:00, 252.29it/s]\n",
      "100%|██████████| 6212/6212 [00:11<00:00, 520.64it/s]\n",
      "100%|██████████| 9289/9289 [00:18<00:00, 502.39it/s]\n",
      "100%|██████████| 4036/4036 [00:12<00:00, 327.89it/s]\n",
      "100%|██████████| 840/840 [00:01<00:00, 638.34it/s] \n",
      "100%|██████████| 823/823 [00:03<00:00, 251.45it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 195.49it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 2833.29it/s]\n",
      "100%|██████████| 8703/8703 [00:11<00:00, 737.70it/s] \n",
      "100%|██████████| 65/65 [00:00<00:00, 191.95it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 63.04it/s]\n",
      "100%|██████████| 259/259 [00:00<00:00, 312.92it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 325.39it/s]\n",
      "100%|██████████| 102/102 [00:00<00:00, 1032.38it/s]\n",
      "100%|██████████| 2708/2708 [00:03<00:00, 739.57it/s]\n",
      "100%|██████████| 62/62 [00:00<00:00, 167.75it/s]\n",
      "100%|██████████| 2488/2488 [00:05<00:00, 485.03it/s]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.65it/s]\n",
      "100%|██████████| 1639/1639 [00:02<00:00, 732.33it/s]\n",
      "100%|██████████| 13018/13018 [00:57<00:00, 225.79it/s]\n",
      "100%|██████████| 24/24 [07:04<00:00, 17.70s/it]\n"
     ]
    }
   ],
   "source": [
    "for dir in tqdm(os.listdir(\".\")):\n",
    "    if not os.path.isdir(dir):\n",
    "        continue\n",
    "    print(dir)\n",
    "    to_csv(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_last_page_to_all_url_pages(last_page_url: str) -> List[str]:\n",
    "    first_page_url = \"?\".join(last_page_url.split(\"?\")[:-1])\n",
    "    nb_pages = int(last_page_url.split(\"=\")[-1]) + 1\n",
    "    all_urls: List[str] = [first_page_url]\n",
    "    all_urls.extend(\n",
    "        first_page_url + \"?page=\" + str(i)\n",
    "        for i in tqdm(range(1, nb_pages), desc=\"Getting url_pages\")\n",
    "    )\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _from_url_page_get_content_urls(url_page: str) -> List[str]:\n",
    "    soup_page = get_soup(url_page)\n",
    "    return [\n",
    "        os.path.join(e[\"about\"][1:])\n",
    "        for e in soup_page.find_all(\n",
    "            \"div\", \"node node-documents node-teaser view-mode-teaser\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def from_page_urls_to_content_urls(url_pages: List[str]) -> List[str]:\n",
    "    content_urls = process_map(\n",
    "        _from_url_page_get_content_urls,\n",
    "        url_pages,\n",
    "        max_workers=MAX_WORKERS,\n",
    "        chunksize=MAX_WORKERS,\n",
    "        desc=\"Getting content urls\",\n",
    "    )\n",
    "    return [url for l in content_urls for url in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _from_content_url_to_info_content(content_url: str) -> dict:\n",
    "    new_url = os.path.join(base_url, content_url)\n",
    "    sub_soup = get_soup(new_url)\n",
    "\n",
    "    author = sub_soup.find(\"h3\", class_=\"diet-title\").text\n",
    "    date = sub_soup.find(\"span\", class_=\"date-display-single\").text\n",
    "    text = sub_soup.find(\"div\", class_=\"field-docs-content\").text\n",
    "    return {\"name\": author, \"date\": date, \"text\": text}\n",
    "\n",
    "\n",
    "def from_content_urls_to_infos_content(content_urls: List[str]) -> List[dict]:\n",
    "    return process_map(\n",
    "        _from_content_url_to_info_content,\n",
    "        content_urls,\n",
    "        max_workers=MAX_WORKERS,\n",
    "        chunksize=MAX_WORKERS,\n",
    "        desc=\"Getting info content\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_infos_in_resources(infos: List[dict], category: str) -> None:\n",
    "    dir_to_save = category\n",
    "    os.makedirs(dir_to_save, exist_ok=True)\n",
    "    for info in tqdm(infos, desc=\"Saving info\"):\n",
    "        title = info[\"name\"] + \"-\" + info[\"date\"] + \".json\"\n",
    "        content = {\"category\": category, \"content\": [info]}\n",
    "        with open(os.path.join(dir_to_save, title), \"w\") as writer:\n",
    "            json.dump(content, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _from_sub_section_to_last_page_url(url: str) -> str:\n",
    "    print(\"Getting last page url\")\n",
    "    soup = get_soup(url)\n",
    "    last = soup.find(\"li\", class_=\"pager-last\")\n",
    "    return os.path.join(base_url, last.find(\"a\")[\"href\"][1:])\n",
    "\n",
    "\n",
    "def from_sub_section_url_save_info(sub_section_url: str) -> None:\n",
    "    category = sub_section_url.split(\"/\")[-1]\n",
    "    if category in os.listdir(\".\"):\n",
    "        print(f\"Category '{category}' : Already in dataset\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing '{category}'\")\n",
    "    last_page_url = _from_sub_section_to_last_page_url(sub_section_url)\n",
    "    url_pages = from_last_page_to_all_url_pages(last_page_url)\n",
    "    content_urls = from_page_urls_to_content_urls(url_pages)\n",
    "    infos = from_content_urls_to_infos_content(content_urls)\n",
    "    save_infos_in_resources(infos, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_menu_to_sub_section_urls(url: str) -> List[str]:\n",
    "    soup = get_soup(url)\n",
    "    dropdown_menu = soup.find(\"li\", class_=\"first expanded menu-mlid-10954 dropdown\")\n",
    "    return [\n",
    "        os.path.join(base_url, child.find(\"a\").attrs[\"href\"][1:])\n",
    "        for child in list(list(dropdown_menu.children)[1].children)\n",
    "        if child != \"\\n\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sub sections:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'eulogies' : Already in dataset\n",
      "Processing 'executive-orders'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 900/900 [00:00<00:00, 544636.21it/s]\n",
      "Getting content urls: 100%|██████████| 901/901 [01:45<00:00,  8.52it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 9003 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 9003/9003 [15:58<00:00,  9.39it/s]\n",
      "Saving info: 100%|██████████| 9003/9003 [00:01<00:00, 5818.77it/s]\n",
      "Sub sections:  10%|▉         | 2/21 [17:48<2:49:08, 534.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'fireside-chats' : Already in dataset\n",
      "Processing 'interviews'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 102/102 [00:00<00:00, 742741.33it/s]\n",
      "Getting content urls: 100%|██████████| 103/103 [00:12<00:00,  8.52it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 1028 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 1028/1028 [01:45<00:00,  9.79it/s]\n",
      "Saving info: 100%|██████████| 1028/1028 [00:00<00:00, 4371.20it/s]\n",
      "Sub sections:  19%|█▉        | 4/21 [19:46<1:12:12, 254.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'letters'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 473/473 [00:00<00:00, 1224633.20it/s]\n",
      "Getting content urls: 100%|██████████| 474/474 [00:53<00:00,  8.87it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 4732 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 4732/4732 [07:59<00:00,  9.87it/s]\n",
      "Saving info: 100%|██████████| 4732/4732 [00:00<00:00, 8016.48it/s]\n",
      "Sub sections:  24%|██▍       | 5/21 [28:42<1:30:04, 337.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'miscellaneous-written'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 10/10 [00:00<00:00, 76398.98it/s]\n",
      "Getting content urls: 100%|██████████| 11/11 [00:10<00:00,  1.09it/s]\n",
      "Getting info content: 100%|██████████| 109/109 [00:11<00:00,  9.16it/s]\n",
      "Saving info: 100%|██████████| 109/109 [00:00<00:00, 5474.81it/s]\n",
      "Sub sections:  29%|██▊       | 6/21 [29:05<1:01:05, 244.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'news-conferences'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 250/250 [00:00<00:00, 1085482.40it/s]\n",
      "Getting content urls: 100%|██████████| 251/251 [00:31<00:00,  8.05it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 2508 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 2508/2508 [04:03<00:00, 10.31it/s]\n",
      "Saving info: 100%|██████████| 2508/2508 [00:00<00:00, 5187.03it/s]\n",
      "Sub sections:  33%|███▎      | 7/21 [33:41<59:14, 253.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'spoken-addresses-and-remarks'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 3232/3232 [00:00<00:00, 1801939.46it/s]\n",
      "Getting content urls: 100%|██████████| 3233/3233 [16:48<00:00,  3.21it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 32329 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 32329/32329 [56:22<00:00,  9.56it/s]\n",
      "Saving info: 100%|██████████| 32329/32329 [00:10<00:00, 3208.50it/s]\n",
      "Sub sections:  38%|███▊      | 8/21 [1:47:07<5:23:36, 1493.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'farewell-address'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 1/1 [00:00<00:00, 7913.78it/s]\n",
      "Getting content urls: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "Getting info content: 100%|██████████| 11/11 [00:01<00:00,  5.65it/s]\n",
      "Saving info: 100%|██████████| 11/11 [00:00<00:00, 3093.14it/s]\n",
      "Sub sections:  43%|████▎     | 9/21 [1:47:13<3:29:43, 1048.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'inaugural-addresses'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 6/6 [00:00<00:00, 34473.73it/s]\n",
      "Getting content urls: 100%|██████████| 7/7 [00:06<00:00,  1.07it/s]\n",
      "Getting info content: 100%|██████████| 62/62 [00:06<00:00,  9.50it/s]\n",
      "Saving info: 100%|██████████| 62/62 [00:00<00:00, 4281.10it/s]\n",
      "Sub sections:  48%|████▊     | 10/21 [1:47:27<2:15:29, 739.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'memoranda'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 347/347 [00:00<00:00, 766012.36it/s]\n",
      "Getting content urls: 100%|██████████| 348/348 [00:39<00:00,  8.81it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 3471 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 3471/3471 [06:16<00:00,  9.21it/s]\n",
      "Saving info: 100%|██████████| 3471/3471 [00:00<00:00, 4968.35it/s]\n",
      "Sub sections:  52%|█████▏    | 11/21 [1:54:26<1:47:10, 643.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'messages'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 1252/1252 [00:00<00:00, 665560.03it/s]\n",
      "Getting content urls: 100%|██████████| 1253/1253 [04:31<00:00,  4.61it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 12528 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 12528/12528 [20:46<00:00, 10.05it/s]\n",
      "Saving info: 100%|██████████| 12528/12528 [00:01<00:00, 6938.17it/s]\n",
      "Sub sections:  57%|█████▋    | 12/21 [2:19:49<2:16:00, 906.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'proclamations'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 928/928 [00:00<00:00, 637875.14it/s]\n",
      "Getting content urls: 100%|██████████| 929/929 [03:15<00:00,  4.74it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 9284 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 9284/9284 [15:15<00:00, 10.14it/s]\n",
      "Saving info: 100%|██████████| 9284/9284 [00:01<00:00, 5596.29it/s]\n",
      "Sub sections:  62%|██████▏   | 13/21 [2:38:25<2:09:15, 969.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'saturday-weekly-addresses-radio'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 163/163 [00:00<00:00, 486945.55it/s]\n",
      "Getting content urls: 100%|██████████| 164/164 [00:20<00:00,  8.05it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 1639 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 1639/1639 [02:35<00:00, 10.51it/s]\n",
      "Saving info: 100%|██████████| 1639/1639 [00:00<00:00, 8549.45it/s]\n",
      "Sub sections:  67%|██████▋   | 14/21 [2:41:22<1:25:24, 732.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'state-dinners'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 25/25 [00:00<00:00, 137789.22it/s]\n",
      "Getting content urls: 100%|██████████| 26/26 [00:09<00:00,  2.83it/s]\n",
      "Getting info content: 100%|██████████| 259/259 [00:25<00:00, 10.15it/s]\n",
      "Saving info: 100%|██████████| 259/259 [00:00<00:00, 7626.70it/s]\n",
      "Sub sections:  71%|███████▏  | 15/21 [2:41:58<52:20, 523.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'state-the-union-addresses'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 9/9 [00:00<00:00, 48395.82it/s]\n",
      "Getting content urls: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n",
      "Getting info content: 100%|██████████| 100/100 [00:09<00:00, 10.04it/s]\n",
      "Saving info: 100%|██████████| 100/100 [00:00<00:00, 3735.58it/s]\n",
      "Sub sections:  76%|███████▌  | 16/21 [2:42:19<31:03, 372.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'state-the-union-written-messages' : Already in dataset\n",
      "Processing 'statements'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 1202/1202 [00:00<00:00, 619507.67it/s]\n",
      "Getting content urls: 100%|██████████| 1203/1203 [04:08<00:00,  4.85it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 12029 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 12029/12029 [19:04<00:00, 10.51it/s]\n",
      "Saving info: 100%|██████████| 12029/12029 [00:01<00:00, 6908.59it/s]\n",
      "Sub sections:  86%|████████▌ | 18/21 [3:05:37<26:09, 523.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'vetoes'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 122/122 [00:00<00:00, 460995.57it/s]\n",
      "Getting content urls: 100%|██████████| 123/123 [00:19<00:00,  6.36it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 1225 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 1225/1225 [02:00<00:00, 10.20it/s]\n",
      "Saving info: 100%|██████████| 1225/1225 [00:00<00:00, 5248.73it/s]\n",
      "Sub sections:  90%|█████████ | 19/21 [3:07:58<14:16, 428.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'citations' : Already in dataset\n",
      "Processing 'written-presidential-orders'\n",
      "Getting last page url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting url_pages: 100%|██████████| 2212/2212 [00:00<00:00, 657068.02it/s]\n",
      "Getting content urls: 100%|██████████| 2213/2213 [10:29<00:00,  3.51it/s]\n",
      "/tmp/ipykernel_26414/3393830896.py:12: TqdmWarning: Iterable length 22124 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n",
      "  return process_map(\n",
      "Getting info content: 100%|██████████| 22124/22124 [37:26<00:00,  9.85it/s]\n",
      "Saving info: 100%|██████████| 22124/22124 [00:03<00:00, 5799.04it/s]\n",
      "Sub sections: 100%|██████████| 21/21 [3:56:02<00:00, 674.39s/it]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.presidency.ucsb.edu/documents\"\n",
    "for sub_section_url in tqdm(from_menu_to_sub_section_urls(url), desc=\"Sub sections\"):\n",
    "    try:\n",
    "        from_sub_section_url_save_info(sub_section_url)\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(e.__traceback__)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
