{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94665c75-ec99-46c5-9f9c-d18504924a01",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a640c47-a19f-4764-81b9-687ef55c3a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/secouss/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List, Any, Tuple\n",
    "\n",
    "import path_imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.read_corpus import read_corpus\n",
    "from gensim.models import KeyedVectors\n",
    "from src.preprocessing.regexp_tokenizer import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b674e0-3e10-4be0-86b4-84e80a0d295a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49dd9de",
   "metadata": {},
   "source": [
    "### Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e3e756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  8.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>Well, thank you very much, Jim, for this oppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>Well, let me talk specifically about what I th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>So all of this is possible. Now, in order for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>Well, I think — let’s talk about taxes, becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>When you add up all the loopholes and deductio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>Every single solitary generation, the dial has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>We’d better be able to do it again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>Well, it could say I’m a lousy candidate, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yeah. And by the way, before I came up here, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>I expect to be there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1416 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     category          name                 date  \\\n",
       "0      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "1      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "2      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "3      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "4      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "...       ...           ...                  ...   \n",
       "2245   debate     Joe Biden                 2020   \n",
       "2247   debate     Joe Biden                 2020   \n",
       "2249   debate     Joe Biden                 2020   \n",
       "2251   debate     Joe Biden                 2020   \n",
       "2253   debate     Joe Biden                 2020   \n",
       "\n",
       "                                                   text  \n",
       "0     Well, thank you very much, Jim, for this oppor...  \n",
       "1     Well, let me talk specifically about what I th...  \n",
       "2     So all of this is possible. Now, in order for ...  \n",
       "3     Well, I think — let’s talk about taxes, becaus...  \n",
       "4     When you add up all the loopholes and deductio...  \n",
       "...                                                 ...  \n",
       "2245  Every single solitary generation, the dial has...  \n",
       "2247                We’d better be able to do it again.  \n",
       "2249  Well, it could say I’m a lousy candidate, and ...  \n",
       "2251  Yeah. And by the way, before I came up here, I...  \n",
       "2253                              I expect to be there.  \n",
       "\n",
       "[1416 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_corpus(categories=[\"debate\"], presidents=[\"Barack Obama\", \"Donald Trump\", \"Joe Biden\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb7220",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f06db9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well, thank you very much, jim, for this opportunity.   i want to thank governor romney and the university of denver for your hospitality.  there are a lot of points i want to make tonight, but the most important one is that 20 years ago i became the luckiest man on earth because michelle obama agreed to marry me.  and so i just want to wish, sweetie, you happy anniversary and let you know that a year from now we will not be celebrating it in front of 40 million people.  you know, four years ago we went through the worst financial crisis since the great depression.   millions of jobs were lost, the auto industry was on the brink of collapse.   the financial system had frozen up.  and because of the resilience and the determination of the american people, we’ve begun to fight our way back.   over the last 30 months, we’ve seen 5 million jobs in the private sector created.   the auto industry has come roaring back.   and housing has begun to rise.  but we all know that we’ve still got a lot of work to do.   and so the question here tonight is not where we’ve been, but where we’re going.  governor romney has a perspective that says if we cut taxes, skewed towards the wealthy, and roll back regulations, that we’ll be better off.   i’ve got a different view.  i think we’ve got to invest in education and training.   i think it’s important for us to develop new sources of energy here in america, that we change our tax code to make sure that we’re helping small businesses and companies that are investing here in the united states, that we take some of the money that we’re saving as we wind down two wars to rebuild america and that we reduce our deficit in a balanced way that allows us to make these critical investments.  now, it ultimately is going to be up to the voters — to you — which path we should take.   are we going to double on top-down economic policies that helped to get us into this mess or do we embrace a new economic patriotism that says america does best when the middle class does best? and i’m looking forward to having that debate.  '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a sapce after each sentence\n",
    "df[\"text\"] = df[\"text\"].str.replace(\".\", \". \", regex=False)\n",
    "\n",
    "# all lower\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# show one word\n",
    "df.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26d4b2",
   "metadata": {},
   "source": [
    "## Word2vec + Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9959058b-b971-49c8-8aa9-f9727c7817ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "#model_word2vec_path = '../word2vec/word2vecBest.model'\n",
    "#word2vec_model: Word2Vec = Word2Vec.load(model_word2vec_path)\n",
    "\n",
    "word2vec_model : KeyedVectors = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f51d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: Callable[[str], List[str]] = RegexpTokenizer().lemma_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb60ba9-fc5e-46bc-adae-d87643ad0891",
   "metadata": {},
   "source": [
    "## Prepare input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096bc439",
   "metadata": {},
   "source": [
    "### Tokenize each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d8c71d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>well, thank you very much, jim, for this oppor...</td>\n",
       "      <td>[well, ,, thank, you, very, much, ,, jim, ,, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>well, let me talk specifically about what i th...</td>\n",
       "      <td>[well, ,, let, me, talk, specifically, about, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>so all of this is possible.   now, in order fo...</td>\n",
       "      <td>[so, all, of, this, is, possible, ., now, ,, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>well, i think — let’s talk about taxes, becaus...</td>\n",
       "      <td>[well, ,, i, think, —, let, ’, s, talk, about,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>debate</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>2012-10-03 09:00:00</td>\n",
       "      <td>when you add up all the loopholes and deductio...</td>\n",
       "      <td>[when, you, add, up, all, the, loophole, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>every single solitary generation, the dial has...</td>\n",
       "      <td>[every, single, solitary, generation, ,, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>we’d better be able to do it again.</td>\n",
       "      <td>[we, ’, d, better, be, able, to, do, it, again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>well, it could say i’m a lousy candidate, and ...</td>\n",
       "      <td>[well, ,, it, could, say, i, ’, m, a, lousy, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>yeah.   and by the way, before i came up here,...</td>\n",
       "      <td>[yeah, ., and, by, the, way, ,, before, i, cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>debate</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>2020</td>\n",
       "      <td>i expect to be there.</td>\n",
       "      <td>[i, expect, to, be, there, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1416 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     category          name                 date  \\\n",
       "0      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "1      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "2      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "3      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "4      debate  Barack Obama  2012-10-03 09:00:00   \n",
       "...       ...           ...                  ...   \n",
       "2245   debate     Joe Biden                 2020   \n",
       "2247   debate     Joe Biden                 2020   \n",
       "2249   debate     Joe Biden                 2020   \n",
       "2251   debate     Joe Biden                 2020   \n",
       "2253   debate     Joe Biden                 2020   \n",
       "\n",
       "                                                   text  \\\n",
       "0     well, thank you very much, jim, for this oppor...   \n",
       "1     well, let me talk specifically about what i th...   \n",
       "2     so all of this is possible.   now, in order fo...   \n",
       "3     well, i think — let’s talk about taxes, becaus...   \n",
       "4     when you add up all the loopholes and deductio...   \n",
       "...                                                 ...   \n",
       "2245  every single solitary generation, the dial has...   \n",
       "2247              we’d better be able to do it again.     \n",
       "2249  well, it could say i’m a lousy candidate, and ...   \n",
       "2251  yeah.   and by the way, before i came up here,...   \n",
       "2253                            i expect to be there.     \n",
       "\n",
       "                                                 tokens  \n",
       "0     [well, ,, thank, you, very, much, ,, jim, ,, f...  \n",
       "1     [well, ,, let, me, talk, specifically, about, ...  \n",
       "2     [so, all, of, this, is, possible, ., now, ,, i...  \n",
       "3     [well, ,, i, think, —, let, ’, s, talk, about,...  \n",
       "4     [when, you, add, up, all, the, loophole, and, ...  \n",
       "...                                                 ...  \n",
       "2245  [every, single, solitary, generation, ,, the, ...  \n",
       "2247  [we, ’, d, better, be, able, to, do, it, again...  \n",
       "2249  [well, ,, it, could, say, i, ’, m, a, lousy, c...  \n",
       "2251  [yeah, ., and, by, the, way, ,, before, i, cam...  \n",
       "2253                      [i, expect, to, be, there, .]  \n",
       "\n",
       "[1416 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens\"] = df['text'].apply(tokenizer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df9334",
   "metadata": {},
   "source": [
    "### Split into n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26fa0d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['well', ',', 'thank', 'you', 'very', 'much', ',', 'jim'],\n",
       "       dtype='<U18'),\n",
       " ',',\n",
       " (74041, 8),\n",
       " (74041,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_sequences(tokens, n) -> Tuple[List[List[Any]], List[Any]]:\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(tokens) - n):\n",
    "        X.append(tokens[i:i+n])\n",
    "        y.append(tokens[i+n])\n",
    "    return X, y\n",
    "\n",
    "NB_GRAM = 8\n",
    "X = []\n",
    "y = []\n",
    "for tokens in df[\"tokens\"]:\n",
    "    v1, v2 = extract_sequences(tokens, NB_GRAM)\n",
    "    X.extend(v1)\n",
    "    y.extend(v2)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X[0], y[0], X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30350f7",
   "metadata": {},
   "source": [
    "### Split into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15855589-ec03-4b08-bb11-bdf74ca229f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59232, 8), (14809, 8), (59232,), (14809,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886beb7e",
   "metadata": {},
   "source": [
    "### Vectorize words (for the input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d747903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59232, 8, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_word(word : str) -> List[float]:\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    return [0] * word2vec_model.vector_size\n",
    "\n",
    "def encode_input(X):\n",
    "    return np.array([[vectorize_word(e) for e in sample] for sample in X])\n",
    "X_train_encoded = encode_input(X_train)\n",
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf74b65",
   "metadata": {},
   "source": [
    "### Encore words to predict (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebe30dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59232, 4075)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder = OneHotEncoder()\n",
    "output_encoder.fit(y.reshape((-1, 1)))\n",
    "def encode_y(y):\n",
    "    return output_encoder.transform(\n",
    "        y.reshape((-1, 1))\n",
    "    ).todense()\n",
    "y_train_encoded = encode_y(y_train)\n",
    "y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ee73e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_GRAM, word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b888186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4075"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_word_voc = np.unique(y).shape[0]\n",
    "nb_word_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfc861",
   "metadata": {},
   "source": [
    "## Create and train our FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6979ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # imports who are working at execution\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.metrics import Precision, Recall\n",
    "    from keras.layers import Dense, Embedding, Flatten, InputLayer\n",
    "    from keras.models import Sequential\n",
    "else: # imports that give information on packages\n",
    "    from tensorflow.python.keras.metrics import Precision, Recall\n",
    "    from tensorflow.python.keras.layers import Dense, Embedding, Flatten, InputLayer\n",
    "    from tensorflow.python.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b8011",
   "metadata": {},
   "source": [
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ea70e6e-1385-41d9-9133-58ad6b3219e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/secouss/repos/nlp-usa-presidents/.venv/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    InputLayer(input_shape=(NB_GRAM, word2vec_model.vector_size)),\n",
    "    Flatten(),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=nb_word_voc, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(learning_rate=0.01)\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[Precision(), Recall()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45168f3d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a66ada54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - loss: 4.9894 - precision_1: 0.9027 - recall_1: 0.0375 - val_loss: 6.0065 - val_precision_1: 0.8569 - val_recall_1: 0.0368\n",
      "Epoch 2/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9525 - precision_1: 0.9245 - recall_1: 0.0397 - val_loss: 5.9285 - val_precision_1: 0.8715 - val_recall_1: 0.0339\n",
      "Epoch 3/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 4.9917 - precision_1: 0.9143 - recall_1: 0.0367 - val_loss: 6.1462 - val_precision_1: 0.7858 - val_recall_1: 0.0352\n",
      "Epoch 4/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9473 - precision_1: 0.9044 - recall_1: 0.0381 - val_loss: 5.8815 - val_precision_1: 0.8317 - val_recall_1: 0.0340\n",
      "Epoch 5/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9763 - precision_1: 0.8899 - recall_1: 0.0368 - val_loss: 5.9587 - val_precision_1: 0.8122 - val_recall_1: 0.0333\n",
      "Epoch 6/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9625 - precision_1: 0.9099 - recall_1: 0.0383 - val_loss: 6.0221 - val_precision_1: 0.8431 - val_recall_1: 0.0348\n",
      "Epoch 7/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 4.9768 - precision_1: 0.9286 - recall_1: 0.0382 - val_loss: 6.2191 - val_precision_1: 0.8443 - val_recall_1: 0.0363\n",
      "Epoch 8/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9809 - precision_1: 0.9192 - recall_1: 0.0381 - val_loss: 6.0259 - val_precision_1: 0.8143 - val_recall_1: 0.0346\n",
      "Epoch 9/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9761 - precision_1: 0.9209 - recall_1: 0.0401 - val_loss: 6.0721 - val_precision_1: 0.8647 - val_recall_1: 0.0354\n",
      "Epoch 10/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9586 - precision_1: 0.9130 - recall_1: 0.0395 - val_loss: 6.0167 - val_precision_1: 0.8574 - val_recall_1: 0.0345\n",
      "Epoch 11/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 4.9696 - precision_1: 0.8925 - recall_1: 0.0381 - val_loss: 6.1599 - val_precision_1: 0.8325 - val_recall_1: 0.0342\n",
      "Epoch 12/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 4.9794 - precision_1: 0.9229 - recall_1: 0.0398 - val_loss: 6.0336 - val_precision_1: 0.8196 - val_recall_1: 0.0310\n",
      "Epoch 13/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 4.9686 - precision_1: 0.9345 - recall_1: 0.0376 - val_loss: 6.0440 - val_precision_1: 0.8630 - val_recall_1: 0.0328\n",
      "Epoch 14/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 4.9610 - precision_1: 0.9182 - recall_1: 0.0391 - val_loss: 6.1827 - val_precision_1: 0.8774 - val_recall_1: 0.0338\n",
      "Epoch 15/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 5.0272 - precision_1: 0.9150 - recall_1: 0.0389 - val_loss: 6.1142 - val_precision_1: 0.8267 - val_recall_1: 0.0351\n",
      "Epoch 16/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9911 - precision_1: 0.9097 - recall_1: 0.0388 - val_loss: 6.2352 - val_precision_1: 0.8375 - val_recall_1: 0.0338\n",
      "Epoch 17/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9924 - precision_1: 0.9096 - recall_1: 0.0380 - val_loss: 6.1908 - val_precision_1: 0.7695 - val_recall_1: 0.0320\n",
      "Epoch 18/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 5.0044 - precision_1: 0.9217 - recall_1: 0.0379 - val_loss: 6.0747 - val_precision_1: 0.8387 - val_recall_1: 0.0337\n",
      "Epoch 19/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9868 - precision_1: 0.9257 - recall_1: 0.0383 - val_loss: 6.1944 - val_precision_1: 0.8411 - val_recall_1: 0.0354\n",
      "Epoch 20/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9818 - precision_1: 0.9410 - recall_1: 0.0398 - val_loss: 6.2489 - val_precision_1: 0.8715 - val_recall_1: 0.0362\n",
      "Epoch 21/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9627 - precision_1: 0.9369 - recall_1: 0.0406 - val_loss: 6.1730 - val_precision_1: 0.8244 - val_recall_1: 0.0352\n",
      "Epoch 22/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9705 - precision_1: 0.9089 - recall_1: 0.0375 - val_loss: 6.3541 - val_precision_1: 0.9082 - val_recall_1: 0.0321\n",
      "Epoch 23/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 4.9833 - precision_1: 0.9155 - recall_1: 0.0377 - val_loss: 6.2062 - val_precision_1: 0.8344 - val_recall_1: 0.0350\n",
      "Epoch 24/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9918 - precision_1: 0.9293 - recall_1: 0.0388 - val_loss: 6.2254 - val_precision_1: 0.8279 - val_recall_1: 0.0328\n",
      "Epoch 25/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 21ms/step - loss: 4.9541 - precision_1: 0.9446 - recall_1: 0.0399 - val_loss: 6.2967 - val_precision_1: 0.8292 - val_recall_1: 0.0364\n",
      "Epoch 26/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 5.0471 - precision_1: 0.9331 - recall_1: 0.0402 - val_loss: 6.0809 - val_precision_1: 0.8554 - val_recall_1: 0.0340\n",
      "Epoch 27/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 4.9474 - precision_1: 0.9303 - recall_1: 0.0404 - val_loss: 6.2227 - val_precision_1: 0.8647 - val_recall_1: 0.0332\n",
      "Epoch 28/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - loss: 5.0374 - precision_1: 0.9167 - recall_1: 0.0389 - val_loss: 6.5213 - val_precision_1: 0.8292 - val_recall_1: 0.0357\n",
      "Epoch 29/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - loss: 5.0034 - precision_1: 0.9469 - recall_1: 0.0397 - val_loss: 6.3530 - val_precision_1: 0.7967 - val_recall_1: 0.0355\n",
      "Epoch 30/30\n",
      "\u001b[1m926/926\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - loss: 4.9551 - precision_1: 0.9235 - recall_1: 0.0407 - val_loss: 6.4213 - val_precision_1: 0.8596 - val_recall_1: 0.0339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7402ec062800>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "model.fit(X_train_encoded, y_train_encoded, epochs=30, batch_size=64, validation_data=(encode_input(X_test), encode_y(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5562c",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5445a084-9fe3-4694-97a1-3049a50b8774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['because' 'he' 'kept' 'worrying' ',' 'in' 'my' 'view']\n",
      ". and , i ’ s a a a the a the the the the a the a the the a the the a a a the the the a a the the the the a the the a the the the a the the the the the the a "
     ]
    }
   ],
   "source": [
    "index = np.random.randint(0, X_train.shape[0])\n",
    "input1 : np.ndarray = X_train[index].copy()\n",
    "all_words = input1.copy().tolist()\n",
    "print(input1)\n",
    "for k in range(50):\n",
    "    res = np.zeros((nb_word_voc))\n",
    "    predictions : np.ndarray = model.predict(encode_input([input1]), verbose = 0)\n",
    "    index = predictions.argsort()[0][-np.random.randint(1, 3)]\n",
    "    res[index] = 1\n",
    "    new_word = output_encoder.inverse_transform([res])[0][0]\n",
    "    print(new_word, end=\" \")\n",
    "    input1[0:-1] = input1[1:]\n",
    "    input1[-1] = new_word\n",
    "    all_words.append(new_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuck : ’ s not the kind of leadership that they do is to simply leave such care insurance for three month , we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure that we ’ ve got to make sure \n",
    "# repeat : ',' 'I' 'Romney' ',' 'I' 'Romney' ',' 'I' 'Romney' ',' 'I' 'Romney' ',' 'I' 'Romney'',' 'I' 'Romney' ',' 'I' 'Romney'\n",
    "# répéter les mots trop fréquents : , when we ’ re not a the a year the . when we think are , a the year of , i make the a year . when they have make . i ’ ’ an a year . that i ’ d , , the , a . \n",
    "# ngram 4 --> grammaticalement bon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
